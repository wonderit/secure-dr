{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed seeds\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #output = output.max(1, keepdim=True)[1]\n",
    "        output = nn.LogSoftmax(dim=1)(output)\n",
    "        #print(\"Target:\",target)\n",
    "        #print(\"output:\",[np.argmax(i) for i in output.cpu().detach().numpy()])\n",
    "        #print(\"Output:\",output)\n",
    "        #print(\"-\"*40)        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = nn.LogSoftmax(dim=1)(output)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            #print(\"Test Predicted:\",pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2346 (0%)]\tLoss: 1.716060\n",
      "Train Epoch: 1 [300/2346 (13%)]\tLoss: 1.290683\n",
      "Train Epoch: 1 [600/2346 (25%)]\tLoss: 1.230907\n",
      "Train Epoch: 1 [900/2346 (38%)]\tLoss: 1.135546\n",
      "Train Epoch: 1 [1200/2346 (51%)]\tLoss: 0.832001\n",
      "Train Epoch: 1 [1500/2346 (63%)]\tLoss: 0.951843\n",
      "Train Epoch: 1 [1800/2346 (76%)]\tLoss: 0.763330\n",
      "Train Epoch: 1 [2100/2346 (89%)]\tLoss: 1.236535\n",
      "\n",
      "Test set: Average loss: 1.8460, Accuracy: 200/733 (27%)\n",
      "\n",
      "Train Epoch: 2 [0/2346 (0%)]\tLoss: 1.051167\n",
      "Train Epoch: 2 [300/2346 (13%)]\tLoss: 1.206290\n",
      "Train Epoch: 2 [600/2346 (25%)]\tLoss: 0.916297\n",
      "Train Epoch: 2 [900/2346 (38%)]\tLoss: 0.881769\n",
      "Train Epoch: 2 [1200/2346 (51%)]\tLoss: 1.130997\n",
      "Train Epoch: 2 [1500/2346 (63%)]\tLoss: 0.729409\n",
      "Train Epoch: 2 [1800/2346 (76%)]\tLoss: 0.735041\n",
      "Train Epoch: 2 [2100/2346 (89%)]\tLoss: 0.911981\n",
      "\n",
      "Test set: Average loss: 1.9742, Accuracy: 200/733 (27%)\n",
      "\n",
      "Train Epoch: 3 [0/2346 (0%)]\tLoss: 0.773660\n",
      "Train Epoch: 3 [300/2346 (13%)]\tLoss: 1.412137\n",
      "Train Epoch: 3 [600/2346 (25%)]\tLoss: 0.845754\n",
      "Train Epoch: 3 [900/2346 (38%)]\tLoss: 1.403769\n",
      "Train Epoch: 3 [1200/2346 (51%)]\tLoss: 0.916681\n",
      "Train Epoch: 3 [1500/2346 (63%)]\tLoss: 0.991269\n",
      "Train Epoch: 3 [1800/2346 (76%)]\tLoss: 1.029071\n",
      "Train Epoch: 3 [2100/2346 (89%)]\tLoss: 0.805064\n",
      "\n",
      "Test set: Average loss: 2.1229, Accuracy: 200/733 (27%)\n",
      "\n",
      "Train Epoch: 4 [0/2346 (0%)]\tLoss: 0.736435\n",
      "Train Epoch: 4 [300/2346 (13%)]\tLoss: 1.034309\n",
      "Train Epoch: 4 [600/2346 (25%)]\tLoss: 0.813001\n",
      "Train Epoch: 4 [900/2346 (38%)]\tLoss: 0.775613\n",
      "Train Epoch: 4 [1200/2346 (51%)]\tLoss: 0.684069\n",
      "Train Epoch: 4 [1500/2346 (63%)]\tLoss: 0.651057\n",
      "Train Epoch: 4 [1800/2346 (76%)]\tLoss: 0.730908\n",
      "Train Epoch: 4 [2100/2346 (89%)]\tLoss: 0.947701\n",
      "\n",
      "Test set: Average loss: 1.8913, Accuracy: 201/733 (27%)\n",
      "\n",
      "Train Epoch: 5 [0/2346 (0%)]\tLoss: 1.058046\n",
      "Train Epoch: 5 [300/2346 (13%)]\tLoss: 0.667052\n",
      "Train Epoch: 5 [600/2346 (25%)]\tLoss: 0.892619\n",
      "Train Epoch: 5 [900/2346 (38%)]\tLoss: 0.785406\n",
      "Train Epoch: 5 [1200/2346 (51%)]\tLoss: 0.710925\n",
      "Train Epoch: 5 [1500/2346 (63%)]\tLoss: 0.848011\n",
      "Train Epoch: 5 [1800/2346 (76%)]\tLoss: 1.084499\n",
      "Train Epoch: 5 [2100/2346 (89%)]\tLoss: 0.860355\n",
      "\n",
      "Test set: Average loss: 1.7865, Accuracy: 215/733 (29%)\n",
      "\n",
      "Train Epoch: 6 [0/2346 (0%)]\tLoss: 0.871245\n",
      "Train Epoch: 6 [300/2346 (13%)]\tLoss: 0.794801\n",
      "Train Epoch: 6 [600/2346 (25%)]\tLoss: 1.008607\n",
      "Train Epoch: 6 [900/2346 (38%)]\tLoss: 0.748524\n",
      "Train Epoch: 6 [1200/2346 (51%)]\tLoss: 1.039963\n",
      "Train Epoch: 6 [1500/2346 (63%)]\tLoss: 0.584363\n",
      "Train Epoch: 6 [1800/2346 (76%)]\tLoss: 0.711410\n",
      "Train Epoch: 6 [2100/2346 (89%)]\tLoss: 0.746707\n",
      "\n",
      "Test set: Average loss: 1.1342, Accuracy: 424/733 (58%)\n",
      "\n",
      "Train Epoch: 7 [0/2346 (0%)]\tLoss: 0.860583\n",
      "Train Epoch: 7 [300/2346 (13%)]\tLoss: 0.960930\n",
      "Train Epoch: 7 [600/2346 (25%)]\tLoss: 0.864477\n",
      "Train Epoch: 7 [900/2346 (38%)]\tLoss: 0.568110\n",
      "Train Epoch: 7 [1200/2346 (51%)]\tLoss: 0.616481\n",
      "Train Epoch: 7 [1500/2346 (63%)]\tLoss: 0.644432\n",
      "Train Epoch: 7 [1800/2346 (76%)]\tLoss: 0.711029\n",
      "Train Epoch: 7 [2100/2346 (89%)]\tLoss: 0.813528\n",
      "\n",
      "Test set: Average loss: 0.7928, Accuracy: 528/733 (72%)\n",
      "\n",
      "Train Epoch: 8 [0/2346 (0%)]\tLoss: 0.625445\n",
      "Train Epoch: 8 [300/2346 (13%)]\tLoss: 0.643013\n",
      "Train Epoch: 8 [600/2346 (25%)]\tLoss: 0.920111\n",
      "Train Epoch: 8 [900/2346 (38%)]\tLoss: 0.406391\n",
      "Train Epoch: 8 [1200/2346 (51%)]\tLoss: 0.841446\n",
      "Train Epoch: 8 [1500/2346 (63%)]\tLoss: 0.612478\n",
      "Train Epoch: 8 [1800/2346 (76%)]\tLoss: 0.867861\n",
      "Train Epoch: 8 [2100/2346 (89%)]\tLoss: 0.737940\n",
      "\n",
      "Test set: Average loss: 0.8083, Accuracy: 518/733 (71%)\n",
      "\n",
      "Train Epoch: 9 [0/2346 (0%)]\tLoss: 0.707098\n",
      "Train Epoch: 9 [300/2346 (13%)]\tLoss: 0.551920\n",
      "Train Epoch: 9 [600/2346 (25%)]\tLoss: 0.716222\n",
      "Train Epoch: 9 [900/2346 (38%)]\tLoss: 1.018256\n",
      "Train Epoch: 9 [1200/2346 (51%)]\tLoss: 0.788399\n",
      "Train Epoch: 9 [1500/2346 (63%)]\tLoss: 0.656198\n",
      "Train Epoch: 9 [1800/2346 (76%)]\tLoss: 0.619399\n",
      "Train Epoch: 9 [2100/2346 (89%)]\tLoss: 0.658297\n",
      "\n",
      "Test set: Average loss: 0.7976, Accuracy: 525/733 (72%)\n",
      "\n",
      "Train Epoch: 10 [0/2346 (0%)]\tLoss: 0.704470\n",
      "Train Epoch: 10 [300/2346 (13%)]\tLoss: 0.596364\n",
      "Train Epoch: 10 [600/2346 (25%)]\tLoss: 0.650472\n",
      "Train Epoch: 10 [900/2346 (38%)]\tLoss: 0.662567\n",
      "Train Epoch: 10 [1200/2346 (51%)]\tLoss: 1.045494\n",
      "Train Epoch: 10 [1500/2346 (63%)]\tLoss: 0.855034\n",
      "Train Epoch: 10 [1800/2346 (76%)]\tLoss: 0.463937\n",
      "Train Epoch: 10 [2100/2346 (89%)]\tLoss: 0.630183\n",
      "\n",
      "Test set: Average loss: 0.7596, Accuracy: 528/733 (72%)\n",
      "\n",
      "Train Epoch: 11 [0/2346 (0%)]\tLoss: 0.731286\n",
      "Train Epoch: 11 [300/2346 (13%)]\tLoss: 0.750737\n",
      "Train Epoch: 11 [600/2346 (25%)]\tLoss: 0.982406\n",
      "Train Epoch: 11 [900/2346 (38%)]\tLoss: 0.685883\n",
      "Train Epoch: 11 [1200/2346 (51%)]\tLoss: 0.666752\n",
      "Train Epoch: 11 [1500/2346 (63%)]\tLoss: 0.403494\n",
      "Train Epoch: 11 [1800/2346 (76%)]\tLoss: 0.722147\n",
      "Train Epoch: 11 [2100/2346 (89%)]\tLoss: 0.649817\n",
      "\n",
      "Test set: Average loss: 0.7753, Accuracy: 527/733 (72%)\n",
      "\n",
      "Train Epoch: 12 [0/2346 (0%)]\tLoss: 0.883159\n",
      "Train Epoch: 12 [300/2346 (13%)]\tLoss: 0.707147\n",
      "Train Epoch: 12 [600/2346 (25%)]\tLoss: 1.047754\n",
      "Train Epoch: 12 [900/2346 (38%)]\tLoss: 0.645850\n",
      "Train Epoch: 12 [1200/2346 (51%)]\tLoss: 0.576541\n",
      "Train Epoch: 12 [1500/2346 (63%)]\tLoss: 0.724998\n",
      "Train Epoch: 12 [1800/2346 (76%)]\tLoss: 0.512304\n",
      "Train Epoch: 12 [2100/2346 (89%)]\tLoss: 0.966684\n",
      "\n",
      "Test set: Average loss: 0.7587, Accuracy: 526/733 (72%)\n",
      "\n",
      "Train Epoch: 13 [0/2346 (0%)]\tLoss: 0.634664\n",
      "Train Epoch: 13 [300/2346 (13%)]\tLoss: 0.635100\n",
      "Train Epoch: 13 [600/2346 (25%)]\tLoss: 0.655561\n",
      "Train Epoch: 13 [900/2346 (38%)]\tLoss: 0.654913\n",
      "Train Epoch: 13 [1200/2346 (51%)]\tLoss: 0.791419\n",
      "Train Epoch: 13 [1500/2346 (63%)]\tLoss: 1.140994\n",
      "Train Epoch: 13 [1800/2346 (76%)]\tLoss: 0.406479\n",
      "Train Epoch: 13 [2100/2346 (89%)]\tLoss: 0.426488\n",
      "\n",
      "Test set: Average loss: 0.7311, Accuracy: 532/733 (73%)\n",
      "\n",
      "Train Epoch: 14 [0/2346 (0%)]\tLoss: 0.709225\n",
      "Train Epoch: 14 [300/2346 (13%)]\tLoss: 0.943248\n",
      "Train Epoch: 14 [600/2346 (25%)]\tLoss: 0.584785\n",
      "Train Epoch: 14 [900/2346 (38%)]\tLoss: 1.039339\n",
      "Train Epoch: 14 [1200/2346 (51%)]\tLoss: 0.748006\n",
      "Train Epoch: 14 [1500/2346 (63%)]\tLoss: 0.733712\n",
      "Train Epoch: 14 [1800/2346 (76%)]\tLoss: 0.889758\n",
      "Train Epoch: 14 [2100/2346 (89%)]\tLoss: 0.691919\n",
      "\n",
      "Test set: Average loss: 0.7344, Accuracy: 531/733 (72%)\n",
      "\n",
      "Train Epoch: 15 [0/2346 (0%)]\tLoss: 0.665822\n",
      "Train Epoch: 15 [300/2346 (13%)]\tLoss: 0.832768\n",
      "Train Epoch: 15 [600/2346 (25%)]\tLoss: 0.580575\n",
      "Train Epoch: 15 [900/2346 (38%)]\tLoss: 0.562806\n",
      "Train Epoch: 15 [1200/2346 (51%)]\tLoss: 0.608297\n",
      "Train Epoch: 15 [1500/2346 (63%)]\tLoss: 0.747627\n",
      "Train Epoch: 15 [1800/2346 (76%)]\tLoss: 0.956282\n",
      "Train Epoch: 15 [2100/2346 (89%)]\tLoss: 0.787163\n",
      "\n",
      "Test set: Average loss: 0.7334, Accuracy: 533/733 (73%)\n",
      "\n",
      "Train Epoch: 16 [0/2346 (0%)]\tLoss: 0.619102\n",
      "Train Epoch: 16 [300/2346 (13%)]\tLoss: 0.747513\n",
      "Train Epoch: 16 [600/2346 (25%)]\tLoss: 0.619640\n",
      "Train Epoch: 16 [900/2346 (38%)]\tLoss: 0.617479\n",
      "Train Epoch: 16 [1200/2346 (51%)]\tLoss: 0.832962\n",
      "Train Epoch: 16 [1500/2346 (63%)]\tLoss: 0.440738\n",
      "Train Epoch: 16 [1800/2346 (76%)]\tLoss: 0.933077\n",
      "Train Epoch: 16 [2100/2346 (89%)]\tLoss: 0.703805\n",
      "\n",
      "Test set: Average loss: 0.7260, Accuracy: 532/733 (73%)\n",
      "\n",
      "Train Epoch: 17 [0/2346 (0%)]\tLoss: 0.657047\n",
      "Train Epoch: 17 [300/2346 (13%)]\tLoss: 0.634190\n",
      "Train Epoch: 17 [600/2346 (25%)]\tLoss: 0.633059\n",
      "Train Epoch: 17 [900/2346 (38%)]\tLoss: 0.639527\n",
      "Train Epoch: 17 [1200/2346 (51%)]\tLoss: 1.310884\n",
      "Train Epoch: 17 [1500/2346 (63%)]\tLoss: 0.495818\n",
      "Train Epoch: 17 [1800/2346 (76%)]\tLoss: 0.469730\n",
      "Train Epoch: 17 [2100/2346 (89%)]\tLoss: 0.719209\n",
      "\n",
      "Test set: Average loss: 0.7278, Accuracy: 530/733 (72%)\n",
      "\n",
      "Train Epoch: 18 [0/2346 (0%)]\tLoss: 0.502911\n",
      "Train Epoch: 18 [300/2346 (13%)]\tLoss: 0.598225\n",
      "Train Epoch: 18 [600/2346 (25%)]\tLoss: 0.612944\n",
      "Train Epoch: 18 [900/2346 (38%)]\tLoss: 0.683320\n",
      "Train Epoch: 18 [1200/2346 (51%)]\tLoss: 0.878365\n",
      "Train Epoch: 18 [1500/2346 (63%)]\tLoss: 0.702882\n",
      "Train Epoch: 18 [1800/2346 (76%)]\tLoss: 0.564281\n",
      "Train Epoch: 18 [2100/2346 (89%)]\tLoss: 0.576905\n",
      "\n",
      "Test set: Average loss: 0.7251, Accuracy: 531/733 (72%)\n",
      "\n",
      "Train Epoch: 19 [0/2346 (0%)]\tLoss: 0.559892\n",
      "Train Epoch: 19 [300/2346 (13%)]\tLoss: 0.913687\n",
      "Train Epoch: 19 [600/2346 (25%)]\tLoss: 0.711010\n",
      "Train Epoch: 19 [900/2346 (38%)]\tLoss: 0.711138\n",
      "Train Epoch: 19 [1200/2346 (51%)]\tLoss: 1.240806\n",
      "Train Epoch: 19 [1500/2346 (63%)]\tLoss: 1.005448\n",
      "Train Epoch: 19 [1800/2346 (76%)]\tLoss: 0.566305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [2100/2346 (89%)]\tLoss: 0.630119\n",
      "\n",
      "Test set: Average loss: 0.7225, Accuracy: 530/733 (72%)\n",
      "\n",
      "Train Epoch: 20 [0/2346 (0%)]\tLoss: 0.683705\n",
      "Train Epoch: 20 [300/2346 (13%)]\tLoss: 0.348976\n",
      "Train Epoch: 20 [600/2346 (25%)]\tLoss: 0.525582\n",
      "Train Epoch: 20 [900/2346 (38%)]\tLoss: 0.557600\n",
      "Train Epoch: 20 [1200/2346 (51%)]\tLoss: 0.522482\n",
      "Train Epoch: 20 [1500/2346 (63%)]\tLoss: 0.652910\n",
      "Train Epoch: 20 [1800/2346 (76%)]\tLoss: 0.656141\n",
      "Train Epoch: 20 [2100/2346 (89%)]\tLoss: 0.801524\n",
      "\n",
      "Test set: Average loss: 0.7225, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 21 [0/2346 (0%)]\tLoss: 0.474922\n",
      "Train Epoch: 21 [300/2346 (13%)]\tLoss: 0.782797\n",
      "Train Epoch: 21 [600/2346 (25%)]\tLoss: 0.476713\n",
      "Train Epoch: 21 [900/2346 (38%)]\tLoss: 0.708365\n",
      "Train Epoch: 21 [1200/2346 (51%)]\tLoss: 0.484219\n",
      "Train Epoch: 21 [1500/2346 (63%)]\tLoss: 0.447614\n",
      "Train Epoch: 21 [1800/2346 (76%)]\tLoss: 0.583475\n",
      "Train Epoch: 21 [2100/2346 (89%)]\tLoss: 0.587450\n",
      "\n",
      "Test set: Average loss: 0.7222, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 22 [0/2346 (0%)]\tLoss: 0.617057\n",
      "Train Epoch: 22 [300/2346 (13%)]\tLoss: 0.383611\n",
      "Train Epoch: 22 [600/2346 (25%)]\tLoss: 0.742975\n",
      "Train Epoch: 22 [900/2346 (38%)]\tLoss: 0.704210\n",
      "Train Epoch: 22 [1200/2346 (51%)]\tLoss: 0.613265\n",
      "Train Epoch: 22 [1500/2346 (63%)]\tLoss: 0.870742\n",
      "Train Epoch: 22 [1800/2346 (76%)]\tLoss: 0.948947\n",
      "Train Epoch: 22 [2100/2346 (89%)]\tLoss: 0.817900\n",
      "\n",
      "Test set: Average loss: 0.7219, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 23 [0/2346 (0%)]\tLoss: 0.613305\n",
      "Train Epoch: 23 [300/2346 (13%)]\tLoss: 0.429943\n",
      "Train Epoch: 23 [600/2346 (25%)]\tLoss: 0.545648\n",
      "Train Epoch: 23 [900/2346 (38%)]\tLoss: 0.881846\n",
      "Train Epoch: 23 [1200/2346 (51%)]\tLoss: 0.690873\n",
      "Train Epoch: 23 [1500/2346 (63%)]\tLoss: 0.589711\n",
      "Train Epoch: 23 [1800/2346 (76%)]\tLoss: 0.877223\n",
      "Train Epoch: 23 [2100/2346 (89%)]\tLoss: 0.583267\n",
      "\n",
      "Test set: Average loss: 0.7217, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 24 [0/2346 (0%)]\tLoss: 0.713406\n",
      "Train Epoch: 24 [300/2346 (13%)]\tLoss: 0.725658\n",
      "Train Epoch: 24 [600/2346 (25%)]\tLoss: 0.522698\n",
      "Train Epoch: 24 [900/2346 (38%)]\tLoss: 0.490989\n",
      "Train Epoch: 24 [1200/2346 (51%)]\tLoss: 0.497124\n",
      "Train Epoch: 24 [1500/2346 (63%)]\tLoss: 0.549277\n",
      "Train Epoch: 24 [1800/2346 (76%)]\tLoss: 1.014964\n",
      "Train Epoch: 24 [2100/2346 (89%)]\tLoss: 0.703822\n",
      "\n",
      "Test set: Average loss: 0.7214, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 25 [0/2346 (0%)]\tLoss: 0.847864\n",
      "Train Epoch: 25 [300/2346 (13%)]\tLoss: 0.716093\n",
      "Train Epoch: 25 [600/2346 (25%)]\tLoss: 0.454029\n",
      "Train Epoch: 25 [900/2346 (38%)]\tLoss: 0.629584\n",
      "Train Epoch: 25 [1200/2346 (51%)]\tLoss: 0.530744\n",
      "Train Epoch: 25 [1500/2346 (63%)]\tLoss: 0.542762\n",
      "Train Epoch: 25 [1800/2346 (76%)]\tLoss: 0.596475\n",
      "Train Epoch: 25 [2100/2346 (89%)]\tLoss: 0.627637\n",
      "\n",
      "Test set: Average loss: 0.7217, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 26 [0/2346 (0%)]\tLoss: 0.468115\n",
      "Train Epoch: 26 [300/2346 (13%)]\tLoss: 0.453317\n",
      "Train Epoch: 26 [600/2346 (25%)]\tLoss: 0.566819\n",
      "Train Epoch: 26 [900/2346 (38%)]\tLoss: 0.946185\n",
      "Train Epoch: 26 [1200/2346 (51%)]\tLoss: 0.827492\n",
      "Train Epoch: 26 [1500/2346 (63%)]\tLoss: 0.682767\n",
      "Train Epoch: 26 [1800/2346 (76%)]\tLoss: 1.053788\n",
      "Train Epoch: 26 [2100/2346 (89%)]\tLoss: 0.542297\n",
      "\n",
      "Test set: Average loss: 0.7209, Accuracy: 528/733 (72%)\n",
      "\n",
      "Train Epoch: 27 [0/2346 (0%)]\tLoss: 0.856836\n",
      "Train Epoch: 27 [300/2346 (13%)]\tLoss: 0.630148\n",
      "Train Epoch: 27 [600/2346 (25%)]\tLoss: 0.763364\n",
      "Train Epoch: 27 [900/2346 (38%)]\tLoss: 0.753340\n",
      "Train Epoch: 27 [1200/2346 (51%)]\tLoss: 0.649920\n",
      "Train Epoch: 27 [1500/2346 (63%)]\tLoss: 0.522121\n",
      "Train Epoch: 27 [1800/2346 (76%)]\tLoss: 0.434326\n",
      "Train Epoch: 27 [2100/2346 (89%)]\tLoss: 0.649848\n",
      "\n",
      "Test set: Average loss: 0.7212, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 28 [0/2346 (0%)]\tLoss: 0.527852\n",
      "Train Epoch: 28 [300/2346 (13%)]\tLoss: 1.017254\n",
      "Train Epoch: 28 [600/2346 (25%)]\tLoss: 0.557389\n",
      "Train Epoch: 28 [900/2346 (38%)]\tLoss: 0.550427\n",
      "Train Epoch: 28 [1200/2346 (51%)]\tLoss: 0.434074\n",
      "Train Epoch: 28 [1500/2346 (63%)]\tLoss: 0.565294\n",
      "Train Epoch: 28 [1800/2346 (76%)]\tLoss: 0.994734\n",
      "Train Epoch: 28 [2100/2346 (89%)]\tLoss: 0.616743\n",
      "\n",
      "Test set: Average loss: 0.7210, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 29 [0/2346 (0%)]\tLoss: 0.572444\n",
      "Train Epoch: 29 [300/2346 (13%)]\tLoss: 0.475828\n",
      "Train Epoch: 29 [600/2346 (25%)]\tLoss: 0.461095\n",
      "Train Epoch: 29 [900/2346 (38%)]\tLoss: 0.473119\n",
      "Train Epoch: 29 [1200/2346 (51%)]\tLoss: 0.643614\n",
      "Train Epoch: 29 [1500/2346 (63%)]\tLoss: 0.742407\n",
      "Train Epoch: 29 [1800/2346 (76%)]\tLoss: 0.633870\n",
      "Train Epoch: 29 [2100/2346 (89%)]\tLoss: 0.947700\n",
      "\n",
      "Test set: Average loss: 0.7211, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 30 [0/2346 (0%)]\tLoss: 0.715461\n",
      "Train Epoch: 30 [300/2346 (13%)]\tLoss: 0.374667\n",
      "Train Epoch: 30 [600/2346 (25%)]\tLoss: 0.750102\n",
      "Train Epoch: 30 [900/2346 (38%)]\tLoss: 0.631810\n",
      "Train Epoch: 30 [1200/2346 (51%)]\tLoss: 0.683603\n",
      "Train Epoch: 30 [1500/2346 (63%)]\tLoss: 0.762560\n",
      "Train Epoch: 30 [1800/2346 (76%)]\tLoss: 0.748118\n",
      "Train Epoch: 30 [2100/2346 (89%)]\tLoss: 1.227832\n",
      "\n",
      "Test set: Average loss: 0.7209, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 31 [0/2346 (0%)]\tLoss: 0.457723\n",
      "Train Epoch: 31 [300/2346 (13%)]\tLoss: 0.779765\n",
      "Train Epoch: 31 [600/2346 (25%)]\tLoss: 1.075281\n",
      "Train Epoch: 31 [900/2346 (38%)]\tLoss: 0.567444\n",
      "Train Epoch: 31 [1200/2346 (51%)]\tLoss: 1.387962\n",
      "Train Epoch: 31 [1500/2346 (63%)]\tLoss: 0.754298\n",
      "Train Epoch: 31 [1800/2346 (76%)]\tLoss: 0.324275\n",
      "Train Epoch: 31 [2100/2346 (89%)]\tLoss: 0.753576\n",
      "\n",
      "Test set: Average loss: 0.7211, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 32 [0/2346 (0%)]\tLoss: 0.772883\n",
      "Train Epoch: 32 [300/2346 (13%)]\tLoss: 0.715740\n",
      "Train Epoch: 32 [600/2346 (25%)]\tLoss: 0.469800\n",
      "Train Epoch: 32 [900/2346 (38%)]\tLoss: 0.792003\n",
      "Train Epoch: 32 [1200/2346 (51%)]\tLoss: 0.310665\n",
      "Train Epoch: 32 [1500/2346 (63%)]\tLoss: 0.438970\n",
      "Train Epoch: 32 [1800/2346 (76%)]\tLoss: 0.644511\n",
      "Train Epoch: 32 [2100/2346 (89%)]\tLoss: 0.708366\n",
      "\n",
      "Test set: Average loss: 0.7213, Accuracy: 528/733 (72%)\n",
      "\n",
      "Train Epoch: 33 [0/2346 (0%)]\tLoss: 0.896585\n",
      "Train Epoch: 33 [300/2346 (13%)]\tLoss: 0.486716\n",
      "Train Epoch: 33 [600/2346 (25%)]\tLoss: 0.755792\n",
      "Train Epoch: 33 [900/2346 (38%)]\tLoss: 0.659706\n",
      "Train Epoch: 33 [1200/2346 (51%)]\tLoss: 0.507074\n",
      "Train Epoch: 33 [1500/2346 (63%)]\tLoss: 0.566032\n",
      "Train Epoch: 33 [1800/2346 (76%)]\tLoss: 0.715875\n",
      "Train Epoch: 33 [2100/2346 (89%)]\tLoss: 0.690003\n",
      "\n",
      "Test set: Average loss: 0.7211, Accuracy: 526/733 (72%)\n",
      "\n",
      "Train Epoch: 34 [0/2346 (0%)]\tLoss: 0.829758\n",
      "Train Epoch: 34 [300/2346 (13%)]\tLoss: 0.605455\n",
      "Train Epoch: 34 [600/2346 (25%)]\tLoss: 0.916248\n",
      "Train Epoch: 34 [900/2346 (38%)]\tLoss: 0.679271\n",
      "Train Epoch: 34 [1200/2346 (51%)]\tLoss: 0.810753\n",
      "Train Epoch: 34 [1500/2346 (63%)]\tLoss: 0.855288\n",
      "Train Epoch: 34 [1800/2346 (76%)]\tLoss: 0.720839\n",
      "Train Epoch: 34 [2100/2346 (89%)]\tLoss: 0.560028\n",
      "\n",
      "Test set: Average loss: 0.7211, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 35 [0/2346 (0%)]\tLoss: 0.613019\n",
      "Train Epoch: 35 [300/2346 (13%)]\tLoss: 1.075724\n",
      "Train Epoch: 35 [600/2346 (25%)]\tLoss: 0.868524\n",
      "Train Epoch: 35 [900/2346 (38%)]\tLoss: 0.487626\n",
      "Train Epoch: 35 [1200/2346 (51%)]\tLoss: 0.603688\n",
      "Train Epoch: 35 [1500/2346 (63%)]\tLoss: 0.858154\n",
      "Train Epoch: 35 [1800/2346 (76%)]\tLoss: 0.357837\n",
      "Train Epoch: 35 [2100/2346 (89%)]\tLoss: 0.752125\n",
      "\n",
      "Test set: Average loss: 0.7209, Accuracy: 528/733 (72%)\n",
      "\n",
      "Train Epoch: 36 [0/2346 (0%)]\tLoss: 0.463444\n",
      "Train Epoch: 36 [300/2346 (13%)]\tLoss: 0.610191\n",
      "Train Epoch: 36 [600/2346 (25%)]\tLoss: 0.654586\n",
      "Train Epoch: 36 [900/2346 (38%)]\tLoss: 0.927310\n",
      "Train Epoch: 36 [1200/2346 (51%)]\tLoss: 0.395473\n",
      "Train Epoch: 36 [1500/2346 (63%)]\tLoss: 0.882865\n",
      "Train Epoch: 36 [1800/2346 (76%)]\tLoss: 0.554343\n",
      "Train Epoch: 36 [2100/2346 (89%)]\tLoss: 0.362615\n",
      "\n",
      "Test set: Average loss: 0.7210, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 37 [0/2346 (0%)]\tLoss: 0.966356\n",
      "Train Epoch: 37 [300/2346 (13%)]\tLoss: 0.505599\n",
      "Train Epoch: 37 [600/2346 (25%)]\tLoss: 0.657906\n",
      "Train Epoch: 37 [900/2346 (38%)]\tLoss: 0.839704\n",
      "Train Epoch: 37 [1200/2346 (51%)]\tLoss: 0.592158\n",
      "Train Epoch: 37 [1500/2346 (63%)]\tLoss: 0.441975\n",
      "Train Epoch: 37 [1800/2346 (76%)]\tLoss: 0.734553\n",
      "Train Epoch: 37 [2100/2346 (89%)]\tLoss: 0.596451\n",
      "\n",
      "Test set: Average loss: 0.7208, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 38 [0/2346 (0%)]\tLoss: 0.670323\n",
      "Train Epoch: 38 [300/2346 (13%)]\tLoss: 0.604860\n",
      "Train Epoch: 38 [600/2346 (25%)]\tLoss: 0.531119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [900/2346 (38%)]\tLoss: 0.539022\n",
      "Train Epoch: 38 [1200/2346 (51%)]\tLoss: 0.631324\n",
      "Train Epoch: 38 [1500/2346 (63%)]\tLoss: 0.523476\n",
      "Train Epoch: 38 [1800/2346 (76%)]\tLoss: 0.609610\n",
      "Train Epoch: 38 [2100/2346 (89%)]\tLoss: 0.540422\n",
      "\n",
      "Test set: Average loss: 0.7213, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 39 [0/2346 (0%)]\tLoss: 0.630865\n",
      "Train Epoch: 39 [300/2346 (13%)]\tLoss: 0.336051\n",
      "Train Epoch: 39 [600/2346 (25%)]\tLoss: 0.534247\n",
      "Train Epoch: 39 [900/2346 (38%)]\tLoss: 0.574930\n",
      "Train Epoch: 39 [1200/2346 (51%)]\tLoss: 0.661987\n",
      "Train Epoch: 39 [1500/2346 (63%)]\tLoss: 0.810320\n",
      "Train Epoch: 39 [1800/2346 (76%)]\tLoss: 0.671461\n",
      "Train Epoch: 39 [2100/2346 (89%)]\tLoss: 0.663562\n",
      "\n",
      "Test set: Average loss: 0.7215, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 40 [0/2346 (0%)]\tLoss: 0.785158\n",
      "Train Epoch: 40 [300/2346 (13%)]\tLoss: 0.725972\n",
      "Train Epoch: 40 [600/2346 (25%)]\tLoss: 0.878055\n",
      "Train Epoch: 40 [900/2346 (38%)]\tLoss: 0.691657\n",
      "Train Epoch: 40 [1200/2346 (51%)]\tLoss: 0.788594\n",
      "Train Epoch: 40 [1500/2346 (63%)]\tLoss: 0.795796\n",
      "Train Epoch: 40 [1800/2346 (76%)]\tLoss: 0.826447\n",
      "Train Epoch: 40 [2100/2346 (89%)]\tLoss: 0.940571\n",
      "\n",
      "Test set: Average loss: 0.7211, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 41 [0/2346 (0%)]\tLoss: 0.552306\n",
      "Train Epoch: 41 [300/2346 (13%)]\tLoss: 0.635059\n",
      "Train Epoch: 41 [600/2346 (25%)]\tLoss: 0.374324\n",
      "Train Epoch: 41 [900/2346 (38%)]\tLoss: 0.475566\n",
      "Train Epoch: 41 [1200/2346 (51%)]\tLoss: 0.868735\n",
      "Train Epoch: 41 [1500/2346 (63%)]\tLoss: 0.758027\n",
      "Train Epoch: 41 [1800/2346 (76%)]\tLoss: 0.589141\n",
      "Train Epoch: 41 [2100/2346 (89%)]\tLoss: 0.651126\n",
      "\n",
      "Test set: Average loss: 0.7215, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 42 [0/2346 (0%)]\tLoss: 0.473438\n",
      "Train Epoch: 42 [300/2346 (13%)]\tLoss: 0.544685\n",
      "Train Epoch: 42 [600/2346 (25%)]\tLoss: 0.982886\n",
      "Train Epoch: 42 [900/2346 (38%)]\tLoss: 0.578282\n",
      "Train Epoch: 42 [1200/2346 (51%)]\tLoss: 1.147410\n",
      "Train Epoch: 42 [1500/2346 (63%)]\tLoss: 0.685526\n",
      "Train Epoch: 42 [1800/2346 (76%)]\tLoss: 0.585921\n",
      "Train Epoch: 42 [2100/2346 (89%)]\tLoss: 0.454325\n",
      "\n",
      "Test set: Average loss: 0.7209, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 43 [0/2346 (0%)]\tLoss: 0.460378\n",
      "Train Epoch: 43 [300/2346 (13%)]\tLoss: 0.592695\n",
      "Train Epoch: 43 [600/2346 (25%)]\tLoss: 0.665651\n",
      "Train Epoch: 43 [900/2346 (38%)]\tLoss: 0.477129\n",
      "Train Epoch: 43 [1200/2346 (51%)]\tLoss: 0.693804\n",
      "Train Epoch: 43 [1500/2346 (63%)]\tLoss: 0.454532\n",
      "Train Epoch: 43 [1800/2346 (76%)]\tLoss: 0.597903\n",
      "Train Epoch: 43 [2100/2346 (89%)]\tLoss: 0.574361\n",
      "\n",
      "Test set: Average loss: 0.7213, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 44 [0/2346 (0%)]\tLoss: 0.594932\n",
      "Train Epoch: 44 [300/2346 (13%)]\tLoss: 0.429607\n",
      "Train Epoch: 44 [600/2346 (25%)]\tLoss: 0.835502\n",
      "Train Epoch: 44 [900/2346 (38%)]\tLoss: 0.689626\n",
      "Train Epoch: 44 [1200/2346 (51%)]\tLoss: 0.473017\n",
      "Train Epoch: 44 [1500/2346 (63%)]\tLoss: 0.420606\n",
      "Train Epoch: 44 [1800/2346 (76%)]\tLoss: 0.593148\n",
      "Train Epoch: 44 [2100/2346 (89%)]\tLoss: 0.380328\n",
      "\n",
      "Test set: Average loss: 0.7217, Accuracy: 530/733 (72%)\n",
      "\n",
      "Train Epoch: 45 [0/2346 (0%)]\tLoss: 0.715295\n",
      "Train Epoch: 45 [300/2346 (13%)]\tLoss: 0.778998\n",
      "Train Epoch: 45 [600/2346 (25%)]\tLoss: 0.757803\n",
      "Train Epoch: 45 [900/2346 (38%)]\tLoss: 0.765196\n",
      "Train Epoch: 45 [1200/2346 (51%)]\tLoss: 0.467030\n",
      "Train Epoch: 45 [1500/2346 (63%)]\tLoss: 0.619738\n",
      "Train Epoch: 45 [1800/2346 (76%)]\tLoss: 0.686381\n",
      "Train Epoch: 45 [2100/2346 (89%)]\tLoss: 0.585076\n",
      "\n",
      "Test set: Average loss: 0.7223, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 46 [0/2346 (0%)]\tLoss: 0.563285\n",
      "Train Epoch: 46 [300/2346 (13%)]\tLoss: 1.060430\n",
      "Train Epoch: 46 [600/2346 (25%)]\tLoss: 0.983341\n",
      "Train Epoch: 46 [900/2346 (38%)]\tLoss: 0.723184\n",
      "Train Epoch: 46 [1200/2346 (51%)]\tLoss: 0.503369\n",
      "Train Epoch: 46 [1500/2346 (63%)]\tLoss: 0.588518\n",
      "Train Epoch: 46 [1800/2346 (76%)]\tLoss: 0.469505\n",
      "Train Epoch: 46 [2100/2346 (89%)]\tLoss: 0.873241\n",
      "\n",
      "Test set: Average loss: 0.7220, Accuracy: 530/733 (72%)\n",
      "\n",
      "Train Epoch: 47 [0/2346 (0%)]\tLoss: 0.553170\n",
      "Train Epoch: 47 [300/2346 (13%)]\tLoss: 0.836444\n",
      "Train Epoch: 47 [600/2346 (25%)]\tLoss: 1.026525\n",
      "Train Epoch: 47 [900/2346 (38%)]\tLoss: 0.647199\n",
      "Train Epoch: 47 [1200/2346 (51%)]\tLoss: 0.588580\n",
      "Train Epoch: 47 [1500/2346 (63%)]\tLoss: 0.539391\n",
      "Train Epoch: 47 [1800/2346 (76%)]\tLoss: 0.759072\n",
      "Train Epoch: 47 [2100/2346 (89%)]\tLoss: 0.427899\n",
      "\n",
      "Test set: Average loss: 0.7212, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 48 [0/2346 (0%)]\tLoss: 0.607239\n",
      "Train Epoch: 48 [300/2346 (13%)]\tLoss: 0.852922\n",
      "Train Epoch: 48 [600/2346 (25%)]\tLoss: 0.744153\n",
      "Train Epoch: 48 [900/2346 (38%)]\tLoss: 0.508764\n",
      "Train Epoch: 48 [1200/2346 (51%)]\tLoss: 0.626603\n",
      "Train Epoch: 48 [1500/2346 (63%)]\tLoss: 0.753966\n",
      "Train Epoch: 48 [1800/2346 (76%)]\tLoss: 0.561295\n",
      "Train Epoch: 48 [2100/2346 (89%)]\tLoss: 0.586531\n",
      "\n",
      "Test set: Average loss: 0.7211, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 49 [0/2346 (0%)]\tLoss: 0.749499\n",
      "Train Epoch: 49 [300/2346 (13%)]\tLoss: 0.654096\n",
      "Train Epoch: 49 [600/2346 (25%)]\tLoss: 0.623753\n",
      "Train Epoch: 49 [900/2346 (38%)]\tLoss: 0.618081\n",
      "Train Epoch: 49 [1200/2346 (51%)]\tLoss: 0.860190\n",
      "Train Epoch: 49 [1500/2346 (63%)]\tLoss: 0.863204\n",
      "Train Epoch: 49 [1800/2346 (76%)]\tLoss: 0.547144\n",
      "Train Epoch: 49 [2100/2346 (89%)]\tLoss: 0.471701\n",
      "\n",
      "Test set: Average loss: 0.7212, Accuracy: 529/733 (72%)\n",
      "\n",
      "Train Epoch: 50 [0/2346 (0%)]\tLoss: 0.477133\n",
      "Train Epoch: 50 [300/2346 (13%)]\tLoss: 0.449434\n",
      "Train Epoch: 50 [600/2346 (25%)]\tLoss: 0.643345\n",
      "Train Epoch: 50 [900/2346 (38%)]\tLoss: 0.587330\n",
      "Train Epoch: 50 [1200/2346 (51%)]\tLoss: 0.625958\n",
      "Train Epoch: 50 [1500/2346 (63%)]\tLoss: 0.488414\n",
      "Train Epoch: 50 [1800/2346 (76%)]\tLoss: 0.849999\n",
      "Train Epoch: 50 [2100/2346 (89%)]\tLoss: 0.628593\n",
      "\n",
      "Test set: Average loss: 0.7208, Accuracy: 528/733 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################### Parameters for training ###################\n",
    "batch_size = 30\n",
    "epochs = 50\n",
    "log_interval = 10\n",
    "save_model = True\n",
    "lr = 0.01\n",
    "gamma = 0.7\n",
    "no_of_classes = 5\n",
    "\n",
    "\n",
    "################### Class weightage ###################\n",
    "class_info  = pd.read_csv(\"./data/train/separated/train.csv\")\n",
    "class_count = class_info['labels'].value_counts().values.tolist()\n",
    "target_list = class_info['labels'].values.tolist()\n",
    "\n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]\n",
    "\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n",
    "class_weights_all = class_weights[target_list]\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "################### Directory for Train and Val ###################\n",
    "traindir = \"./data/train/separated/train\"\n",
    "valdir = \"./data/train/separated/val\"\n",
    "\n",
    "################### Defining GPU ###################\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "################### Physical parameters for training ###################\n",
    "kwargs = {'batch_size': batch_size}\n",
    "if use_cuda:\n",
    "    kwargs.update({'num_workers': 1,\n",
    "                   'pin_memory': True},\n",
    "                 )\n",
    "\n",
    "################### Defining transformations for image and dataset object ###################\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.ImageFolder(traindir,transform=transform)\n",
    "dataset2 = datasets.ImageFolder(valdir,transform=transform)\n",
    "\n",
    "################### Defining train and val dataloader ###################\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, sampler=weighted_sampler, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
    "\n",
    "################### Defining modified network ###################\n",
    "model = EfficientNet.from_name('efficientnet-b1')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model._fc.in_features\n",
    "model._fc = nn.Linear(num_ftrs, no_of_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "################### Criterion, Optimizer and Scheduler ###################\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "################### Main ###################\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "################### Save model ###################\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), \"aptos_blindness_effnet_b1_weighted.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model_1.pt\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = EfficientNet.from_name('efficientnet-b1').to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 71.8696397941681 %\n"
     ]
    }
   ],
   "source": [
    "testdir = \"./data/train/separated/test\"\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.ImageFolder(testdir,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset1)\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "test_loss = 0\n",
    "\n",
    "true_labels = []\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        true_labels.append(labels)\n",
    "        output = model(images)\n",
    "        output = nn.LogSoftmax(dim=1)(output)\n",
    "        test_loss += F.nll_loss(output, labels, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        preds.append(pred)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [i.cpu().detach().numpy()[0][0] for i in preds]\n",
    "true_labels = [i.cpu().detach().numpy()[0] for i in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[273   1  14   0   0]\n",
      " [  3   5  51   0   0]\n",
      " [ 13   5 141   0   0]\n",
      " [  0   0  30   0   0]\n",
      " [  2   3  42   0   0]]\n",
      "Accuracy score : 0.7186963979416809\n",
      "kappa score : 0.5469763519116051\n"
     ]
    }
   ],
   "source": [
    "conf = confusion_matrix(true_labels, preds)\n",
    "print(conf)\n",
    "print(\"Accuracy score : \"+str(accuracy_score(true_labels, preds)))\n",
    "print(\"kappa score : \"+str(cohen_kappa_score(true_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
